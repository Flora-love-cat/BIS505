---
title: "Lab 5 BIS 505b"
author: "Maria Ciarleglio"
date: "3/22/2021"
output: 
    html_document:
        toc: true
        highlight: default
---

<!--- Copyright (c) 2021, M. Ciarleglio --->

<!--- Set global options --->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
options(scipen=999)               # global option that requests no scientific notation in this session
options(show.signif.stars=FALSE)  # global option to hide significance stars 
```

<!--- Load packages --->
```{r, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(car)
library(multcomp)
library(MASS)
```

<!--- Set WD and read data file --->
```{r, include=FALSE}
#setwd("C:\\BIS_505\\LABS\\Lab5")  # modify path to your R Lab5 folder
nhanes <- read.csv("nhanes.csv")
```


# Goal of Lab 5

In **Lab 5**, we will **(1)** work with categorical predictors, **(2)** create multiple linear regression models, **(3)** explore interactions between variables and **(4)** present automated variable selection methods. 


# Analysis Data Set

In this lab, we will analyze a subset of data from the **National Health and Nutrition Examination Survey** (NHANES) ($n$ = 1430) `nhanes.csv` imported as the data frame `nhanes` in code chunk 3 above). The **Data Key** is provided below:


| Variable Name	| Definition |
|---------------|------------|
|`fastgluc` | Fasting glucose level (mg/dL) (**Our Response**) |
|           | &nbsp; 88888 = Missing |
|`age`      | Age at time of survey |
|`sex`      | Sex |
|           | &nbsp; 0 = Male |
|           | &nbsp; 1 = Female |
|`oralmed`  | Oral diabetes medication use |
|           | &nbsp; 0 = No |
|           | &nbsp; 1 = Yes |
|`race`     | Race/Ethnicity |
|           | &nbsp; 1 = White |
|           | &nbsp; 2 = Black |
|           | &nbsp; 3 = Mexican-American |
|           | &nbsp; 4 = Other |

After reviewing the Data Key, we see that missing values of `fastgluc` are coded as `88888`. Begin by re-coding this numerical value of `88888` as `NA` in **R**. 

```{r}
# Re-code a `fastgluc` value of 88888 as NA
nhanes$fastgluc[nhanes$fastgluc == 88888] <- NA

# Checking missing values
summary(nhanes)
```


- **Missing Values:**

*Note*: `fastgluc` contains `r sum(is.na(nhanes$fastgluc))` **missing values**. These observations will not be used to fit any of the models. In addition, individuals with a missing value for any of the explanatory variables included in a specific model will not be used to fit that model. This is an important consideration when thinking about which explanatory variables to include. For example, including a predictor that has a large proportion of missing values in the model will consequently exclude anyone who is missing a value for that variable from the regression model. This is called a **complete case analysis**. That is, only records with complete data on all variables included in a model, `y ~ x1 + x2 + x3`, (complete data for `y`, `x1`, `x2` and `x3`) will be analyzed.


- **Creating Factor Variables:**

There are several **categorical variables** in this data set (`sex`, `oralmed` and `race`). Use the `str()` function to see how each variable is stored in **R** (numeric, integer, factor, character). 

```{r}
str(nhanes)
```

Notice that the variables for race, oral diabetes medication use and sex are coded as integers, but should be coded as **factor variables**. We can use either the `mutate()` function in the `dplyr` package or the `factor()` function directly to redefine these variables as factors based on the Data Key.

```{r}
# Creating factor variables in nhanes, mutate() function in the "dplyr" package
nhanes <- mutate(nhanes,
                 sex_factor = factor(sex,
                                     levels = c(0, 1),
                                     labels = c("Male", "Female")),
                 oralmed_factor = factor(oralmed,
                                         levels = c(0, 1),
                                         labels = c("No", "Yes")),
                 race_factor = factor(race,
                                      levels = c(1, 2, 3, 4), 
                                      labels = c("White", "Black", "Mexican-American", "Other")))

str(nhanes)
```

*Note*: When we include **factor variables** in our regression model, the **first level** specified in the `factor()` function is used as the **reference level**. For example, given the way `sex_factor` is defined above (`levels = c(0, 1)`), a linear regression model that includes `sex_factor` will assume *male* is the reference category. The effect of `sex_factor` then compares females (1) to males (0) (reference). Re-ordering the `levels=` argument of the `factor()` function allows us to specify the desired reference level by listing it first. For example, to report the average difference in fasting glucose level in males (0) vs. females (1) (reference), define `sex_factor` as `factor(sex, levels = c(1, 0), labels = c("Female", "Male"))`.


# Research Questions

We are interested in studying characteristics associated with **fasting glucose levels** `fastgluc` (our response variable, $y$). The **research questions** include:

  1. Is there evidence of an association between *age* and *fasting glucose levels*, after controlling for *sex*, *oral diabetes medication use* and *race/ethnicity*?  

  2. In the multiple regression model, is *race/ethnicity* an important predictor of *fasting glucose level*?  

  3. Does the effect of *age* on *fasting glucose levels* differ by *oral diabetes medication use*?  


# Multiple Linear Regression

**Multiple linear regression** is used to describe the relationship between a quantitative response variable $y$ and more than one explanatory variable $x_1, \ldots, x_k$. The *population regression model*, $\mu_{y|x} = \alpha + \beta_1 x_1 + \ldots + \beta_k x_k$, is estimated using the method of **least squares**, giving a *fitted model*, $\hat{y} = a + b_1 x_1 + \ldots + b_k x_k$. 

- The fitted model is used to **predict** or **estimate** the expected value of $y$ for given values of $x_1, \ldots, x_k$ by plugging these values of $x$ into the fitted equation, $\hat{y} = a + b_1 x_1 + \ldots + b_k x_k$.
- The estimated slope $b_j$ is used to **describe the association** between $x_j$ and $y$, controlling for or holding all other explanatory variables constant. 

A test of the slope, $\beta_j$ (i.e., $H_0: \beta_j = 0$ vs. $H_1: \beta_j \neq 0$) is used to determine if an  association exists between $x_j$ and $y$, holding all other explanatory variables constant. This test is performed using the t-statistic, $t = \frac{b_j}{s_{b_j}}$, which is compared to a $t$-distribution with $n-p$ *degrees of freedom*. Note that $p$ is equal to the number of regression parameters estimated in the model ($k$ slopes + 1 intercept), so $p = k+1$.

The `lm()` function in **R** is used to estimate the regression coefficients (i.e., the intercept and slope parameter(s)) of the linear model. The result of the `lm()` function is usually saved as an object (e.g., `regobject`) and the `summary()` function is applied to that object (`summary(regobject)`) to output detailed results.

`lm()` Function Arguments | Option Definition
---|---------
`formula=` | `analysis_variable ~ predictor_variable1 + predictor_variable2` |
`data=`    | Data frame containing sample data |

A $100(1-\alpha)$% confidence interval for the parameter $\beta_j$ is equal to $b_j \pm t_{1-\alpha/2; n-p} \, s_{b_j}$. The `confint(regobject)` function is used to return 95% confidence intervals for the model parameters. By default, 95% confidence intervals (`level=0.95`) are produced.

Finally, the fitted model can be used to estimate or predict a value of $y$ for given values of $x$ using the `predict()` function. A new data frame must be created and specified in the `newdata=` argument of the `predict()` function that contains the values of $x$ used to predict values of $y$. 


## Binary Predictor Variable

So far, have only considered **quantitative predictor variables**. **Categorical variables** can be included as predictors in a regression model through the use of numeric 0/1 **dummy** or **indicator variables** $z_j$, where the reference level of the variable equals `0`.

When we include a **factor variable** in a regression model, **R** will automatically create the dummy variable(s) necessary to represent that categorical variable. The `contrasts()` function returns the dummy variable coding that **R** uses to represent a factor variable. For example, `sex_factor` is a dummy variable ($z_1$) that equals `1` for females  and `0` for males (the reference category). 

```{r}
contrasts(nhanes$sex_factor)
```

- In an **unadjusted model** that contains `sex_factor`, $\hat{y} = a + b_1 z_1$, the estimated slope of `sex_factor` $b_1$ equals the estimated *difference* in mean fasting glucose in females vs. males (ref) ($\bar{y}_f - \bar{y}_m$). Including additional variables in the model will give *adjusted* differences in mean fasting glucose.

```{r}
# SLR model including sex_factor
mod.sex <- lm(fastgluc ~ sex_factor, data = nhanes)
summary(mod.sex)

# Mean of fastgluc by sex_factor
mn <- aggregate(x = list(fastgluc.mean = nhanes$fastgluc), 
                by = list(sex = nhanes$sex_factor),
                FUN = mean, 
                na.rm = TRUE)
mn
```

- The **fitted model** is given by the equation, $\hat{y} =$ `r round(summary(mod.sex)$coefficients[1],2)` `r ifelse(summary(mod.sex)$coefficients[2]<0,"$-$","$+$")` `r round(abs(summary(mod.sex)$coefficients[2]),2)` Female. To make the interpretation of the fitted model easier, instead of using the variable name `sex_factor` in the written fitted model, I specified the level of the dummy variable that is being compared to the reference level (i.e., female vs. male). 

- The **estimated intercept** $a =$ `r round(summary(mod.sex)$coefficients[1],2)` is equal to the mean `fastgluc` when $z_1 = 0$ (i.e., the mean fasting glucose in the reference category (males)).

- The **estimated slope** of `sex_factor` $b_1 =$ `r round(summary(mod.sex)$coefficients[2],2)` is equal to the *difference* in mean `fastgluc` in females (`r round(mn$fastgluc.mean[2],2)`) minus males (`r round(mn$fastgluc.mean[1],2)`).

- A **significance test of the slope** ($H_0: \beta_1=0$ vs. $\beta_1 \neq 0$) reports a t-statistic t =`r round(summary(mod.sex)$coefficients[6],2)`, which is compared to a $t$-distribution with `r summary(mod.sex)$df[2]` degrees of freedom. This test does not support a significant difference in the mean fasting glucose in males vs. females (p-value `r ifelse(summary(mod.sex)$coefficients[8]<.001,"<.001",paste("=",round(summary(mod.sex)$coefficients[8],3)))`). 

You can specify the reference category when creating a factor variable by listing that category as the **first level** in the `levels=` argument of the `factor()` function. For example, `sex_factorv2` will set *female* as the reference category. Notice that the dummy variable for `sex_factorv2` equals `1` for males and `0` for females.

```{r}
# Female will be the reference category (=0) of sex_factorv2
nhanes$sex_factorv2 <- factor(nhanes$sex, 
                              levels = c(1, 0), 
                              labels = c("Female", "Male"))
contrasts(nhanes$sex_factorv2)
```

The estimated slope of `sex_factorv2` is equal to the estimated difference in mean fasting glucose in males vs. females (ref) ($\bar{y}_m - \bar{y}_f$). 

```{r}
# SLR model including sex_factorv2
mod.sexv2 <- lm(fastgluc ~ sex_factorv2, data = nhanes)
summary(mod.sexv2)
```

We can also use the `relevel()` function to change the reference category (`ref=`) of an existing factor variable:

```{r}
# Female will be the reference category (=0) of sex_factorv3
nhanes$sex_factorv3 <- relevel(nhanes$sex_factor, ref = "Female")
contrasts(nhanes$sex_factorv3)
```


## Categorical Predictor Variable

**Categorical variables** with $C$ levels are represented by a set of $C-1$ dummy variables. Again, when using factor versions of our categorical variables, **R** automatically creates the dummy variables needed to represent the categorical variable in a regression model. Be sure **not** to use `ordered = TRUE` when creating factor variables for inclusion in a regression model. 

`race_factor` contains **4** levels (White, Black, Mexican-American, and Other) and must be represented by **3** dummy variables ($z_1$, $z_2$ and $z_3$). When we created `race_factor` at the beginning of this Lab, White was specified as the first level (`levels=c(1,2,3,4)` corresponding to `labels=c("White", "Black", "Mexican-American", "Other")`), thus `"White"` will be the reference category. All dummy variables will equal `0` for the reference level of the categorical variable. Below, we see the 3 dummy variables that describe race:

```{r}
contrasts(nhanes$race_factor)
```

  1. $z_1$ equals `1` when `race_factor == "Black"` and equals `0` otherwise  

  2. $z_2$ equals `1` when `race_factor == "Mexican-American"` and equals `0` otherwise  

  3. $z_3$ equals `1` when `race_factor == "Other"` and equals `0` otherwise  

- In an **unadjusted model**, $\hat{y} = a + b_1 z_1 + b_2 z_2 + b_3 z_3$, the estimated slope of the first dummy variable $b_1$ equals the estimated difference in mean fasting glucose in Blacks vs. Whites (ref) ($\bar{y}_b - \bar{y}_w$). The estimated slope of the second dummy variable $b_2$ equals the estimated difference in mean fasting glucose in Mexican-Americans vs. Whites (ref) ($\bar{y}_m - \bar{y}_w$). The estimated slope of the third dummy variable $b_3$ equals the estimated difference in mean fasting glucose in Others vs. Whites (ref) ($\bar{y}_o - \bar{y}_w$). Including additional variables in the model will give *adjusted* differences in mean fasting glucose.

```{r}
# Model including race_factor
mod.race <- lm(fastgluc ~ race_factor, data = nhanes)
summary(mod.race)
```

- The **fitted model** is given by the equation, $\hat{y} =$ `r round(summary(mod.race)$coefficients[1],2)` `r ifelse(summary(mod.race)$coefficients[2]<0,"$-$","$+$")` `r round(abs(summary(mod.race)$coefficients[2]),2)` Black `r ifelse(summary(mod.race)$coefficients[3]<0,"$-$","$+$")` `r round(abs(summary(mod.race)$coefficients[3]),2)` Mexican-American `r ifelse(summary(mod.race)$coefficients[4]<0,"$-$","$+$")` `r round(abs(summary(mod.race)$coefficients[4]),2)` Other. 

- The average fasting glucose in Blacks is $b_1 =$ `r round(summary(mod.race)$coefficients[2],2)` [95% CI (`r round(confint(mod.race)[2,1],2)`, `r round(confint(mod.race)[2,2],2)`)] units higher than in Whites. A **significance test** of $\beta_1$ shows that there is a significant difference in the mean fasting glucose in Blacks vs. Whites (p-value `r ifelse(summary(mod.race)$coefficients[14]<.001,"<.001",paste("=",round(summary(mod.race)$coefficients[14],3)))`).  

- The average fasting glucose in Mexican-Americans is $b_2 =$ `r round(summary(mod.race)$coefficients[3],2)` [95% CI (`r round(confint(mod.race)[3,1],2)`, `r round(confint(mod.race)[3,2],2)`)] units higher than in Whites. A **significance test** of $\beta_2$ shows that there is a significant difference in the mean fasting glucose in Mexican-Americans vs. Whites (p-value `r ifelse(summary(mod.race)$coefficients[15]<.001,"<.001",paste("=",round(summary(mod.race)$coefficients[15],3)))`).  

- The average fasting glucose in Other races/ethnicities is $b_3 =$ `r round(summary(mod.race)$coefficients[4],2)` [95% CI (`r round(confint(mod.race)[4,1],2)`, `r round(confint(mod.race)[4,2],2)`)] units different (lower) than in Whites. A **significance test** of $\beta_3$ shows that there is not a significant difference in the mean fasting glucose in Other races/ethnicities vs. Whites (p-value `r ifelse(summary(mod.race)$coefficients[16]<.001,"<.001",paste("=",round(summary(mod.race)$coefficients[16],3)))`).  


## The Unadjusted Model

The first research question asks if there is an association between *age* (`age`) and *fasting glucose levels* (`fastgluc`), after controlling for *sex* (`sex_factor`), *oral diabetes medication use* (`oralmed_factor`) and *race/ethnicity* (`race_factor`). Let's begin by estimating the **unadjusted effect** of `age` on `fastgluc` using a **simple linear regression model**. We call this the *unadjusted effect* because no other variables are being controlled for or adjusted for in the regression model.

- **Unadjusted model**: $\mu_{y|x} = \alpha + \beta_1$ Age

We fit the model using the `lm()` function and save the fitted model to the object `mod.age`. We output a summary of the results using `summary(mod.age)` and the 95\% CIs of the model parameters using `confint(mod.age)`.

```{r}
# SLR model including age
mod.age <- lm(fastgluc ~ age, data = nhanes)

# Output results of fitted model
summary(mod.age)

# Confidence intervals for model parameters (intercept and slope)
confint(mod.age)
```

- The **fitted model** is given by the equation, $\hat{y} =$ `r round(summary(mod.age)$coefficients[1],2)` `r ifelse(summary(mod.age)$coefficients[2]<0,"$-$","$+$")` `r round(abs(summary(mod.age)$coefficients[2]),2)` Age.

- The **estimated slope** of `age` $b_1 =$ `r round(summary(mod.age)$coefficients[2],2)` [95% CI (`r round(confint(mod.age)[2,1],2)`, `r round(confint(mod.age)[2,2],2)`)] indicates that a 1-unit increase in age is associated with a `r round(summary(mod.age)$coefficients[2],2)`-unit average change (a decrease) in fasting glucose. 

- A **significance test of the slope** ($H_0: \beta_1=0$ vs. $\beta_1 \neq 0$) reports a t-statistic t =`r round(summary(mod.age)$coefficients[6],2)`, which is compared to a $t$-distribution with `r summary(mod.age)$df[2]` degrees of freedom. This yields a highly significant p-value `r ifelse(summary(mod.age)$coefficients[8]<.001,"<.001",paste("=",round(summary(mod.age)$coefficients[8],3)))`.

- The **R-squared** of this model is low at `r round(summary(mod.age)$r.squared,3)`, indicating that `age` only explains `r round(100*summary(mod.age)$r.squared,1)`% of the total variability in fasting glucose. The variability about the regression line $\sigma_{y|x}$ is estimated by the "**residual standard error**" in the output above and is equal to $s_{y|x}$ = `r round(summary(mod.age)$sigma, 2)`. 


## The Adjusted Model

To **control** or **adjust for** *sex*, *oral diabetes medication use* and *race/ethnicity* when estimating the effect of *age* on *fasting glucose*, we will fit a **multiple linear regression model** that additionally includes the variables `sex_factor`, `oralmed_factor` and `race_factor`. 

- **Adjusted model**: $\mu_{y|x} = \alpha + \beta_1$ Age $+ \beta_2$ Female $+ \beta_3$ MedUse $+ \beta_4$ Black $+ \beta_5$ Mexican-American $+ \beta_6$ Other

```{r}
# MLR model including age, sex_factor, oralmed_factor, race_factor
mod.mlr <- lm(fastgluc ~ age + sex_factor + oralmed_factor + race_factor, data = nhanes)
summary(mod.mlr)
confint(mod.mlr)
```

- The **fitted model** is given by the equation, $\hat{y} =$ `r round(summary(mod.mlr)$coefficients[1],2)` `r ifelse(summary(mod.mlr)$coefficients[2]<0,"$-$","$+$")` `r round(abs(summary(mod.mlr)$coefficients[2]),2)` Age `r ifelse(summary(mod.mlr)$coefficients[3]<0,"$-$","$+$")` `r round(abs(summary(mod.mlr)$coefficients[3]),2)` Female `r ifelse(summary(mod.mlr)$coefficients[4]<0,"$-$","$+$")` `r round(abs(summary(mod.mlr)$coefficients[4]),2)` MedUse `r ifelse(summary(mod.mlr)$coefficients[5]<0,"$-$","$+$")` `r round(abs(summary(mod.mlr)$coefficients[5]),2)` Black `r ifelse(summary(mod.mlr)$coefficients[6]<0,"$-$","$+$")` `r round(abs(summary(mod.mlr)$coefficients[6]),2)` Mexican-American `r ifelse(summary(mod.mlr)$coefficients[7]<0,"$-$","$+$")` `r round(abs(summary(mod.mlr)$coefficients[7]),2)` Other.

- Sex, medication use, and race-adjusted effect of **age** on fasting glucose:

  - The **estimated slope** of `age` $b_1 =$ `r round(summary(mod.mlr)$coefficients[2],2)` [95% CI (`r round(confint(mod.mlr)[2,1],2)`, `r round(confint(mod.mlr)[2,2],2)`)] in the multiple linear regression model indicates that a 1-unit increase in age is associated with a `r round(summary(mod.mlr)$coefficients[2],2)`-unit average change (a decrease) in fasting glucose, controlling for sex, oral diabetes medication use and race. 
  
  - A **significance test of the slope** ($H_0: \beta_1=0$ vs. $\beta_1 \neq 0$) shows a highly significant association between age and fasting glucose when controlling for sex, oral diabetes medication use and race (p-value `r ifelse(summary(mod.mlr)$coefficients[23]<.001,"<.001",paste("=",round(summary(mod.mlr)$coefficients[23],3)))`).  
\

- Age, medication use, and race-adjusted effect of **sex** on fasting glucose:

  - The **estimated slope** of `sex_factor` $b_2 =$ `r round(summary(mod.mlr)$coefficients[3],2)` [95% CI (`r round(confint(mod.mlr)[3,1],2)`, `r round(confint(mod.mlr)[3,2],2)`)] estimates the average difference fasting glucose in females vs. males (reference), controlling for age, oral diabetes medication use and race. The adjusted average fasting glucose level is `r round(summary(mod.mlr)$coefficients[3],2)`-units higher in females than in males.
  
  - A **significance test of the slope** ($H_0: \beta_2=0$ vs. $\beta_2 \neq 0$) shows there is not a statistically significant difference in average fasting glucose in females and males when controlling for age, oral diabetes medication use and race (p-value `r ifelse(summary(mod.mlr)$coefficients[24]<.001,"<.001",paste("=",round(summary(mod.mlr)$coefficients[24],3)))`). Notice that the 95% confidence interval for $\beta_2$ supports this conclusion since it includes 0 (i.e., the value of $\beta_2$ hypothesized under $H_0$).
  
  - Since sex is not a significant predictor in the presence of age, oral diabetes medication use and race, we may want to remove this variable from the regression model. However, if **confounding** is a concern, we can retain the variable regardless of statistical significance.  
\

- The **Adjusted R-squared** ($R^2_a$) of this model remains low at `r round(summary(mod.mlr)$adj.r.squared,3)`. The **residual standard error** is equal to $s_{y|x}$ = `r round(summary(mod.mlr)$sigma, 2)`, and is only slightly smaller than the estimate from the unadjusted model. 

----

<span style="color: #2980B9;">**Exercise**: Interpret the effect of oral diabetes medication use in the model above.
</span> 

----

<details><summary>Answer:</summary>

> Age, sex, and race-adjusted effect of **oral diabetes medication use** on fasting glucose:  

> The **estimated slope** of `oralmed_factor` $b_3 =$ `r round(summary(mod.mlr)$coefficients[4],2)` [95% CI (`r round(confint(mod.mlr)[4,1],2)`, `r round(confint(mod.mlr)[4,2],2)`)] compares the average fasting glucose in those who use oral diabetes medication vs. those who do not (reference), controlling for age, sex and race. The adjusted average fasting glucose level is `r round(summary(mod.mlr)$coefficients[4],2)`-units higher in those who use oral diabetes medication than those who do not. 

> A **significance test of the slope** ($H_0: \beta_3=0$ vs. $\beta_3 \neq 0$) shows there is not a statistically significant difference in average fasting glucose in those who use oral diabetes medication and those who do not when controlling for age, sex and race (p-value `r ifelse(summary(mod.mlr)$coefficients[25]<.001,"<.001",paste("=",round(summary(mod.mlr)$coefficients[25],3)))`). 
</details>
\

----

<span style="color: #2980B9;">**Exercise**: Interpret the effect of race/ethnicity in the model above.
</span> 

----

<details><summary>Answer:</summary>

> Age, sex and medication use-adjusted effect of **race/ethnicity** on fasting glucose:

> The average fasting glucose in Blacks is $b_4 =$ `r round(summary(mod.mlr)$coefficients[5],2)` [95% CI (`r round(confint(mod.mlr)[5,1],2)`, `r round(confint(mod.mlr)[5,2],2)`)] units higher than in Whites, controlling for age, sex and oral diabetes medication use. A **significance test** of $\beta_4$ shows that there is a significant difference in the mean fasting glucose in Blacks vs. Whites (p-value `r ifelse(summary(mod.mlr)$coefficients[26]<.001,"<.001",paste("=",round(summary(mod.mlr)$coefficients[26],3)))`).  

> The average fasting glucose in Mexican-Americans is $b_5 =$ `r round(summary(mod.mlr)$coefficients[6],2)` [95% CI (`r round(confint(mod.mlr)[6,1],2)`, `r round(confint(mod.mlr)[6,2],2)`)] units higher than in Whites, controlling for age, sex and oral diabetes medication use. Unlike in the unadjusted model, a **significance test** of $\beta_5$ shows that there is **not** a significant difference in the mean fasting glucose in Mexican-Americans vs. Whites (p-value `r ifelse(summary(mod.mlr)$coefficients[27]<.001,"<.001",paste("=",round(summary(mod.mlr)$coefficients[27],3)))`).  

> The average fasting glucose in Other races/ethnicities is $b_6 =$ `r round(summary(mod.mlr)$coefficients[7],2)` [95% CI (`r round(confint(mod.mlr)[7,1],2)`, `r round(confint(mod.mlr)[7,2],2)`)] units different (lower) than in Whites, controlling for age, sex and oral diabetes medication use. A **significance test** of $\beta_6$ shows that there is not a significant difference in the mean fasting glucose in Other races/ethnicities vs. Whites (p-value `r ifelse(summary(mod.mlr)$coefficients[28]<.001,"<.001",paste("=",round(summary(mod.mlr)$coefficients[28],3)))`). 
</details>
\


## Overall $F$-Test

The **Overall F-Test** tests whether the explanatory variables collectively have an effect on the response variable. Under $H_0$, $\beta_1 = \beta_2 = \ldots = \beta_k = 0$. Under $H_1$, at least one $\beta_j \neq 0$ for $j = 1, \ldots, k$. The **F-test statistic** is equal to the ratio of the variability explained by the *model* (MSM) to the mean squared *error* (MSE), giving $F = \frac{MSM}{MSE}$. The **F-test statistic** is compared to an $F$-distribution with *numerator degrees of freedom* $k$ and *denominator degrees of freedom* $n - p$. 


- **Option 1:** The overall F-test is presented in the last line of the `summary()` of model results.

```{r}
# Overall F-test on last line of output
summary(mod.mlr)
```

The **overall F-test** F-statistic, F = `r round(summary(mod.mlr)$fstatistic[1],3)`, is compared to an $F$-distribution with `r summary(mod.mlr)$fstatistic[2]` and `r summary(mod.mlr)$fstatistic[3]` degrees of freedom, giving a p-value `r ifelse(as.numeric(1-pf(summary(mod.mlr)$fstatistic[1], summary(mod.mlr)$fstatistic[2], summary(mod.mlr)$fstatistic[3]))<.001,"<.001",paste("=",round(as.numeric(1-pf(summary(mod.mlr)$fstatistic[1], summary(mod.mlr)$fstatistic[2], summary(mod.mlr)$fstatistic[3])),3)))`. There is evidence to conclude that a significant association exists between fasting glucose level and at least one explanatory variable in the MLR model.


- **Option 2:** We can request the ANOVA table of a model using the `anova()` function on the model object (e.g., `mod.mlr`). Note that the ANOVA table has the contribution to the sum of squares broken down by predictor (“Sequential SS” or "Type I SS") and does **not** provide the overall F-statistic. To find the Model SS ($SSM$), add the sum of squares for all predictors. To find the $MSM$, divide $SSM$ by the number of independent variables, $k$, which is the number of parameters tested under $H_0$ (i.e., 6 in this case; recall that race is represented by 3 dummy variables).

```{r}
# ANOVA table
anova(mod.mlr)
```

The **overall F-test** F-statistic, F = [(`r round(anova(mod.mlr)$"Sum Sq"[1],0)` + `r round(anova(mod.mlr)$"Sum Sq"[2],0)` + `r round(anova(mod.mlr)$"Sum Sq"[3],0)` + `r round(anova(mod.mlr)$"Sum Sq"[4],0)`)/(`r anova(mod.mlr)$Df[1]` + `r anova(mod.mlr)$Df[2]` + `r anova(mod.mlr)$Df[3]` + `r anova(mod.mlr)$Df[4]`)]/`r round(anova(mod.mlr)$"Mean Sq"[5],0)` = `r round((anova(mod.mlr)$"Sum Sq"[1]+anova(mod.mlr)$"Sum Sq"[2]+anova(mod.mlr)$"Sum Sq"[3]+anova(mod.mlr)$"Sum Sq"[4])/(anova(mod.mlr)$Df[1]+anova(mod.mlr)$Df[2]+anova(mod.mlr)$Df[3]+anova(mod.mlr)$Df[4])/anova(mod.mlr)$"Mean Sq"[5],3)` agrees with the F-statistic reported in the `summary()` output.


- **Option 3:** Finally, we can perform the overall F-test using the `anova()` function to compare two **nested models** using the syntax `anova(reducedmodel, fullmodel)`.

  - **Full model**: $\mu_{y|x} = \alpha + \beta_1$ Age $+ \beta_2$ Female $+ \beta_3$ MedUse $+ \beta_4$ Black $+ \beta_5$ Mexican-American $+ \beta_6$ Other  
  - **Reduced model** (i.e., model under $H_0$): $\mu_{y|x} = \alpha$ 

Under the reduced model, $H_0$ is assumed to be true (i.e., $\beta_1 = \beta_2 = \ldots = \beta_6 = 0$). When performing the overall F-test, the reduced model is also known as the **null model** since it contains only the intercept term and no explanatory variables. The **R** syntax for fitting a model that contains only the intercept is `yvariable ~ 1`, where `1` represents the intercept.

*Note*: Recall that each regression model only includes records with **complete data** on all variables included in that model (**complete case analysis**). Since there are some individuals with missing values for `oralmed_factor`, the sample size used to fit `mod.full` is *smaller* than the sample size used to fit `mod.null`. To compare the same subset of observations under both the full and reduced models, we must specify that the analysis data set used to fit `mod.full` should also be used to fit `mod.null`. The "complete case" data frame used to fit `mod.full` is `data = mod.full$model` and is specified as the data source for `mod.null`: 

```{r}
# Full model
mod.full <- lm(fastgluc ~ age + sex_factor + oralmed_factor + race_factor, data = nhanes)

# Reduced model (null model) fit using the same observations used to fit full model
mod.null <- lm(fastgluc ~ 1, data = mod.full$model)  # note **data=** here!

# F-test comparing full and reduced models
anova(mod.null, mod.full)
```

The **overall F-test** F-statistic, F = [`r round(anova(mod.null, mod.full)$"Sum of Sq"[2],0)`/`r anova(mod.null, mod.full)$Df[2]`]/[`r round(anova(mod.null, mod.full)$"RSS"[2],0)`/`r anova(mod.null, mod.full)$Res.Df[2]`] = `r round(anova(mod.null, mod.full)$F[2],3)` agrees with the F-statistic reported in the `summary()` output.


## Partial $F$-Test

The **Partial F-Test** simultaneously tests the significance of a group or set of parameters. This test is commonly used to test the effect of categorical variables that are naturally made up of more than one dummy variable. For example, to test the significance of **race/ethnicity** in the adjusted model, we would test: $H_0: \beta_4 = \beta_5 = \beta_6 = 0$ vs. $H_1: \beta_4$, $\beta_5$, $\beta_6$ not all 0.

Here, we are comparing two **nested models**

  - **Full model**: $\mu_{y|x} = \alpha + \beta_1$ Age $+ \beta_2$ Female $+ \beta_3$ MedUse $+ \beta_4$ Black $+ \beta_5$ Mexican-American $+ \beta_6$ Other  
  - **Reduced model** (i.e., model under $H_0$, without `race_factor`): $\mu_{y|x} = \alpha + \beta_1$ Age $+ \beta_2$ Female $+ \beta_3$ MedUse 

The **partial F-test statistic** is equal to $\text{F}_0 = \frac{\frac{SSM(F) - SSM(R)}{\text{Number parameters tested under } H_0}}{\frac{SSE(F)}{df_2(F)}}$.

The F-statistic is compared to an $F$-distribution with *numerator degrees of freedom* equal to the number of parameters tested under $H_0$ and *denominator degrees of freedom* $n - p$.


- **Option 1:** We can perform the partial F-test using the `anova()` function to compare two **nested models** using the syntax `anova(reducedmodel, fullmodel)`.

As above, we need to be sure that we are fitting both the full and reduced models using the same data set. Thus, when fitting the *reduced model*, use the observations that were included in the full model by specifying `data=mod.full$model`.

```{r}
# Full model
mod.full <- lm(fastgluc ~ age + sex_factor + oralmed_factor + race_factor, data = nhanes)

# Reduced model (under H0, does not include race_factor)
# Fit using the same observations included in the full model
mod.red <- lm(fastgluc ~ age + sex_factor + oralmed_factor, data = mod.full$model)

# F-test comparing full and reduced models
anova(mod.red, mod.full)
```

The **partial F-test** F-statistic, F = [`r round(anova(mod.red, mod.full)$"Sum of Sq"[2],0)`/`r anova(mod.red, mod.full)$Df[2]`]/[`r round(anova(mod.red, mod.full)$"RSS"[2],0)`/`r anova(mod.red, mod.full)$Res.Df[2]`] = `r round(anova(mod.red, mod.full)$F[2],3)` is compared to an $F$-distribution with `r anova(mod.red, mod.full)$Df[2]` and `r anova(mod.red, mod.full)$Res.Df[2]` degrees of freedom. The effect of race is not statistically significant in the full model (p = `r round(anova(mod.red, mod.full)$"Pr(>F)"[2],3)`), thus we cannot reject $H_0$. Although we did see that $\beta_4$ (coefficient for dummy variable of Black vs. White) was significantly different from 0 in the individual t-tests of the slopes in `mod.mlr`, perhaps the effect was not strong enough to outweigh the lack of statistical significance seen in the other two dummy variables that make up `race_factor`. 


- **Option 2:** Option 1 is a more flexible option for carrying out a partial F-test and be used to simultaneously test many different slope parameters involving *different* variables (e.g., simultaneously test the effect of `age` and `sex_factor` by testing $H_0: \beta_1=\beta_2=0$). However, if the goal of the partial F-test is to test $C-1$ dummy variables of a *single* $C$-level categorical variable, then we can use the `Anova()` function in the `car` package. The `Anova()` function applied to a model object (e.g., `mod.full`) returns individual F-tests for each variable in the model. A reduced model does not need to be explicitly specified in Option 2. 

To test the effect of `race_factor` in a model containing age, sex and oral diabetes medication use, $\mu_{y|x} = \alpha + \beta_1$ Age $+ \beta_2$ Female $+ \beta_3$ MedUse $+ \beta_4$ Black $+ \beta_5$ Mexican-American $+ \beta_6$ Other, we would test $H_0: \beta_4 = \beta_5 = \beta_6 = 0$ vs. $H_1: \beta_4$, $\beta_5$, $\beta_6$ not all 0. 

```{r}
mod.full <- lm(fastgluc ~ age + sex_factor + oralmed_factor + race_factor, data = nhanes)

# Anova() function in the "car" package
Anova(mod.full)
```

Based on the output above, the **partial F-test** of all dummy variables that make up `race_factor` has an F-statistic = [`r round(Anova(mod.full)$"Sum Sq"[4],0)`/`r Anova(mod.full)$Df[4]`]/[`r round(Anova(mod.full)$"Sum Sq"[5],0)`/`r Anova(mod.full)$Df[5]`] = `r round(Anova(mod.full)$"F value"[4],3)`, which is compared to an $F$-distribution with `r Anova(mod.full)$Df[4]` and `r Anova(mod.full)$Df[5]` degrees of freedom. As we saw above, the effect of race is not statistically significant in the presence of the other variables (p = `r round(Anova(mod.full)$"Pr(>F)"[4],3)`), thus we cannot reject $H_0$.


## Interactions

Next, we would like to determine if the effect of *age* on *fasting glucose level* differs by *oral diabetes medication use*. Answering this question involves examining the **interaction** between `age` and `oralmed_factor`. The model that includes the interaction `age*oralmed_factor` also includes the main effects of `age` and `oralmed_factor`, 

- **Interaction model**: $\mu_{y|x} = \alpha + \beta_1$ Age $+ \beta_2$ MedUse $+ \beta_3$ Age $\times$ MedUse
- Model in those who use oral diabetes medication (MedUse = 1): $\mu_{y|x} = (\alpha + \beta_2) + (\beta_1 + \beta_3)$ Age 
- Model in those who do not use oral diabetes medication (reference) (MedUse = 0): $\mu_{y|x} = \alpha + \beta_1$ Age 

Thus, a test of $\beta_3$ will determine if there is a significant interaction between age and oral diabetes medication use. If a significant interaction exists, then we can estimate the slope of age (i.e., the effect of age on fasting glucose level) separately in the two medication use categories. Note that for simplicity, I am examining this interaction in a model that does not control for sex or race. However, you could easily also include these variables in the model to examine the interaction while controlling for sex and race.

```{r}
# Interaction model of age*oralmed_factor
mod.intx <- lm(fastgluc ~ age + oralmed_factor + age*oralmed_factor, data = nhanes)
summary(mod.intx)
confint(mod.intx)
```

- The **fitted model** is given by the equation, $\hat{y} =$ `r round(summary(mod.intx)$coefficients[1],2)` `r ifelse(summary(mod.intx)$coefficients[2]<0,"$-$","$+$")` `r round(abs(summary(mod.intx)$coefficients[2]),2)` Age `r ifelse(summary(mod.intx)$coefficients[3]<0,"$-$","$+$")` `r round(abs(summary(mod.intx)$coefficients[3]),2)` MedUse `r ifelse(summary(mod.intx)$coefficients[4]<0,"$-$","$+$")` `r round(abs(summary(mod.intx)$coefficients[4]),2)` Age $\times$ MedUse. 

- The **estimated slope** of the interaction `age*oralmed_factor` $b_3 =$ `r round(summary(mod.intx)$coefficients[4],2)` [95% CI (`r round(confint(mod.intx)[4,1],2)`, `r round(confint(mod.intx)[4,2],2)`)] is equal to the *difference* in the slope of age in those who use oral diabetes medication vs. those who do not (ref). 

- A **significance test of the interaction term** ($H_0: \beta_3=0$ vs. $\beta_3 \neq 0$) reports a t-statistic t =`r round(summary(mod.intx)$coefficients[12],2)`, which is compared to a $t$-distribution with `r summary(mod.intx)$df[2]` degrees of freedom. This test supports a significant difference in the effect of age on fasting glucose level in those who use oral diabetes medication vs. those who do not (p-value `r ifelse(summary(mod.intx)$coefficients[16]<.001,"<.001",paste("=",round(summary(mod.intx)$coefficients[16],3)))`). 

The effect (slope) of age on fasting glucose level in those who use oral diabetes medication is estimated by $b_1+b_3$; the effect (slope) of age in those who do not use oral diabetes medication is estimated by $b_1$. We can use **R** to compute the slope in the medication use group and perform a hypothesis test to determine if age significantly affects fasting glucose level in those who use oral diabetes medication by testing $H_0: \beta_1 + \beta_3 = 0$ vs. $H_1: \beta_1 + \beta_3 \neq 0$.

We can estimate this slope and test this **linear contrast** by using the `glht()` function in the `multcomp` package. We begin by specifying `K`, which identifies the coefficients that are involved in the estimation (i.e., $b_1 + b_3$), or `c(0, 1, 0, 1)`. We then specify `K` in the `linfct=` argument of the `glht()` function to specify the linear hypothesis to be tested. `summary()` returns the estimate of the effect and the hypothesis test results; `confint()` returns a confidence interval for the effect.

```{r}
# b1 + b3: Effect of age in those with oralmed_factor = 1

# Vector that specifies linear combination of coefficients interested in
K <- rbind(c(0, 1, 0, 1))   # 1 = coefficients "on" when estimating slope in oralmed_factor = 1

# Label for comparison (printed in the output)
rownames(K) <- "b1+b3 (slope in oralmed_factor = 1)"

# Estimate of slope (b1 + b3) and hypothesis test, glht() function in the "multcomp" package
summary(glht(mod.intx, linfct = K))

# Confidence interval for beta1 + beta3
confint(glht(mod.intx, linfct = K))
```

- Effect of **age** on fasting glucose levels in those who *use oral diabetes medication* is estimated by $b_1 + b_3 =$ `r round(summary(glht(mod.intx, linfct = K))$test$coefficients,2)` [95% CI (`r round(confint(glht(mod.intx, linfct = K))$confint[2],2)`, `r round(confint(glht(mod.intx, linfct = K))$confint[3],2)`)]. We have evidence to reject $H_0: \beta_1+\beta_3=0$ and conclude that there is a significant association between age and fasting blood glucose in those who use oral diabetes medication (p-value `r ifelse(summary(glht(mod.intx, linfct=K))$test$pvalues<.001,"<.001",paste("=",round(summary(glht(mod.intx, linfct=K))$test$pvalues,3)))`). 

- Effect of **age** on fasting glucose levels in those who *do not use oral diabetes medication* is estimated by $b_1=$ `r round(summary(mod.intx)$coefficients[2],2)` [95% CI (`r round(confint(mod.intx)[2,1],2)`, `r round(confint(mod.intx)[2,2],2)`)]. We do not have evidence to reject $H_0: \beta_1=0$ and cannot conclude that there is a significant association between age and fasting blood glucose in those who do not use oral diabetes medication (p-value `r ifelse(summary(mod.intx)$coefficients[14]<.001,"<.001",paste("=",round(summary(mod.intx)$coefficients[14],3)))`).

- The **estimated slope** of the interaction $b_3 =$ `r round(summary(mod.intx)$coefficients[4],2)` is equal to the *difference* in the slope of age in those who use oral diabetes medication vs. those who do not (ref). The test of the interaction is telling us that there is a significant difference in the effect of age in these two groups.


# Automated Variable Selection

**Automated variable selection methods** have been developed to choose the "best-fitting" model (i.e., the “best” subset of predictors). There are three automated variable selection procedures:

  1. **Backward elimination** begins with the full model and iteratively removes predictors that contribute least to the model until all variables remaining exceed a certain threshold. Once a variable is removed, it cannot re-enter the model. Backward elimination tends to be helpful if you have a modest-sized model and would like to eliminate a few predictors.

  2. **Forward selection** begins with the null model (no predictors) and iteratively adds the most important predictors, stopping when there the amount of improvement is below a certain threshold. Once a variable is entered into the model, it cannot be removed. Forward selection tends to be helpful if you have a large set of potential predictors and wish to identify a few important variables.

  3. **Stepwise selection** is a combination of forward selection and backward elimination. This method begins with the null model and iteratively adds predictors. After each addition, there is the option of removing any of the variables already included if removing that variable improves the model fit.

Automated selection methods can be based variable p-value thresholds or other model fit statistics, such as $R^2_a$. The **Akaike Information Criterion** (**AIC**) and the **Bayes Information Criterion** (**BIC**) are other commonly used criteria. The goal is to *minimize* AIC and BIC (i.e., smaller AIC and BIC is better). Just like $R^2_a$, both of these statistics penalize larger models and will not automatically decrease when additional variables are added to the model. 

The `stepAIC()` function in the `MASS` package can be used to conduct automated variable selection based on the AIC. The **null model** (intercept only model) and the **full model** (model that includes all candidate predictors) must be defined.

`stepAIC()` Function Arguments | Option Definition
---|---------
`object=`    | Model object (full model for backward elimination, null model for forward and stepwise selection) |
`scope=`     | Range of models `=list(lower = nullmodel, upper = fullmodel)` |
`direction=` | `=both` (stepwise), `=backward` (backward), `=forward` (forward) |

Again, to avoid error messages about missing observations resulting in different data sets used in the null model and the full model, we fit the null model using the observations included in the full model `data=mod.full$model`.

```{r}
# Full model (contains all predictors under consideration)
mod.full <- lm(fastgluc ~ age + sex_factor + oralmed_factor + race_factor, data = nhanes)

# Null model (intercept only, notice data= here)
mod.null <- lm(fastgluc ~ 1, data = mod.full$model)
```

- **Backward elimination** stops when the AIC does not improve (i.e., does not decrease) after removing a predictor.

```{r}
# Backward elimination, stepAIC() function in the "MASS" package
stepAIC(mod.full, scope = list(lower = mod.null, upper = mod.full), 
        data = nhanes, direction = 'backward')
```

- **Forward selection** stops when the AIC does not improve after adding a predictor.

```{r}
# Forward selection
stepAIC(mod.null, scope = list(lower = mod.null, upper = mod.full), 
        data = nhanes, direction = 'forward')
```

- **Stepwise selection** stops when the AIC does not improve after potentially adding or removing a predictor.

```{r}
# Stepwise selection
stepAIC(mod.null, scope = list(lower = mod.null, upper = mod.full), 
        data = nhanes, direction = 'both')
```

All three selection procedures exclude sex from the selected model but retain age, oral diabetes medication use and race. We consistently saw that sex was not an important predictor of fasting glucose levels. The selected model is given by,

```{r}
# Selected model
mod.selected <- lm(fastgluc ~ age + oralmed_factor + race_factor, data = nhanes)
summary(mod.selected)
```

- The final **fitted model** is given by the equation, $\hat{y} =$ `r round(summary(mod.selected)$coefficients[1],2)` `r ifelse(summary(mod.selected)$coefficients[2]<0,"$-$","$+$")` `r round(abs(summary(mod.selected)$coefficients[2]),2)` Age `r ifelse(summary(mod.selected)$coefficients[3]<0,"$-$","$+$")` `r round(abs(summary(mod.selected)$coefficients[3]),2)` MedUse `r ifelse(summary(mod.selected)$coefficients[4]<0,"$-$","$+$")` `r round(abs(summary(mod.selected)$coefficients[4]),2)` Black `r ifelse(summary(mod.selected)$coefficients[5]<0,"$-$","$+$")` `r round(abs(summary(mod.selected)$coefficients[5]),2)` Mexican-American `r ifelse(summary(mod.selected)$coefficients[6]<0,"$-$","$+$")` `r round(abs(summary(mod.selected)$coefficients[6]),2)` Other.

Notice that the variable `oralmed_factor` is included in the final model despite not being "statistically significant" at the $\alpha=0.05$-level. Not all variables must be statistically significant to contribute to the model. In addition, you can refine the model further by again exploring the interaction of age and oral medication use or other plausible interactions in this model. I recommend not relying solely on automated variable selection methods to choose a final model. Rather, use your subject-area knowledge to help you build and refine a final model that makes sense in your application.  


